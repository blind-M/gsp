# Create a Streaming Data Lake on Cloud Storage: Challenge Lab|| [ARC110](https://www.cloudskillsboost.google/course_templates/705/labs/461631) ||
---
## ⚠️ **Important Note:**
This guide is provided to support your educational journey in this lab. Please open and review each step of the script to gain full understanding. Be sure to follow the terms of Qwiklabs and YouTube’s guidelines as you proceed.

---
### Run the following Commands in cloud shall
```
#!/bin/bash
clear
echo -n "Enter TOPIC Name: "
read TOPIC_ID
echo "You entered TOPIC Name: $TOPIC_ID"
echo
echo -n "Enter MESSAGE: "
read MESSAGE
echo "You entered MESSAGE: $MESSAGE"
echo
echo -n "Enter REGION: "
read REGION
echo "You entered REGION: $REGION"
echo
PROJECT_ID=$(gcloud config get-value project)
echo "Using PROJECT_ID: $PROJECT_ID"

export BUCKET_NAME="${PROJECT_ID}-bucket"
echo "Bucket Name will be: $BUCKET_NAME"

echo "Disabling Dataflow API if already enabled..."
gcloud services disable dataflow.googleapis.com

echo "Enabling required APIs: Dataflow and Cloud Scheduler..."
gcloud services enable dataflow.googleapis.com
gcloud services enable cloudscheduler.googleapis.com

echo "Creating a Cloud Storage bucket: gs://$BUCKET_NAME"
gsutil mb gs://$BUCKET_NAME

echo "Creating Pub/Sub topic: $TOPIC_ID"
gcloud pubsub topics create $TOPIC_ID

echo "Creating App Engine application in region: $REGION"
gcloud app create --region=$REGION

echo "Waiting for App Engine setup to complete..."
sleep 100

echo "Creating a Cloud Scheduler job to publish messages to the topic..."
gcloud scheduler jobs create pubsub arcadecrew --schedule="* * * * *" \
  --topic=$TOPIC_ID --message-body="$MESSAGE"

echo "Waiting for the Scheduler job to be ready..."
sleep 20

echo "Running the Cloud Scheduler job manually for testing..."
gcloud scheduler jobs run arcadecrew

echo "Creating a script to run Pub/Sub to GCS pipeline..."
cat > run_pubsub_to_gcs_arcadecrew.sh <<EOF_CP
#!/bin/bash

# Set environment variables
export PROJECT_ID=$PROJECT_ID
export REGION=$REGION
export TOPIC_ID=$TOPIC_ID
export BUCKET_NAME=$BUCKET_NAME

# Clone the repository and navigate to the required directory
git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git
cd python-docs-samples/pubsub/streaming-analytics

# Install dependencies
pip install -U -r requirements.txt

# Run the Python script with parameters
python PubSubToGCS.py \
  --project=$PROJECT_ID \
  --region=$REGION \
  --input_topic=projects/$PROJECT_ID/topics/$TOPIC_ID \
  --output_path=gs://$BUCKET_NAME/samples/output \
  --runner=DataflowRunner \
  --window_size=2 \
  --num_shards=2 \
  --temp_location=gs://$BUCKET_NAME/temp
EOF_CP

chmod +x run_pubsub_to_gcs_arcadecrew.sh

echo "Running the Pub/Sub to GCS pipeline script inside a Docker container..."
docker run -it \
  -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID \
  -v "$(pwd)/run_pubsub_to_gcs_arcadecrew.sh:/run_pubsub_to_gcs_arcadecrew.sh" \
  python:3.7 \
  /bin/bash -c "/run_pubsub_to_gcs_arcadecrew.sh"
  ```

### Congratulations 🎉 for completing the Lab !