# Stream Processing with Cloud Pub/Sub and Dataflow: Qwik Start|| [GSP903](https://www.cloudskillsboost.google/course_templates/705/labs/461630) ||
---
## âš ï¸ **Important Note:**
This guide is provided to support your educational journey in this lab. Please open and review each step of the script to gain full understanding. Be sure to follow the terms of Qwiklabs and YouTubeâ€™s guidelines as you proceed.

---
```
#!/bin/bash

clear
echo "Enter the region:"
read -p "Region: " REGION
export REGION=$REGION
gcloud config set compute/region $REGION

echo "Disabling and enabling the Dataflow API to ensure proper configuration..."
gcloud services disable dataflow.googleapis.com
gcloud services enable dataflow.googleapis.com

echo "Enabling the Cloud Scheduler API..."
gcloud services enable cloudscheduler.googleapis.com
sleep 60

PROJECT_ID=$(gcloud config get-value project)
BUCKET_NAME="${PROJECT_ID}-bucket"
TOPIC_ID=my-id

echo "Creating a Cloud Storage bucket named $BUCKET_NAME..."
gsutil mb gs://$BUCKET_NAME

echo "Creating a Pub/Sub topic named $TOPIC_ID..."
gcloud pubsub topics create $TOPIC_ID

echo "Setting up App Engine for your project..."
if [ "$REGION" == "us-central1" ]; then
  gcloud app create --region us-central
elif [ "$REGION" == "europe-west1" ]; then
  gcloud app create --region europe-west
else
  gcloud app create --region "$REGION"
fi

echo "Creating a Cloud Scheduler job to publish messages to the Pub/Sub topic..."
gcloud scheduler jobs create pubsub publisher-job --schedule="* * * * *" \
    --topic=$TOPIC_ID --message-body="Hello!"
sleep 90

echo "Running the Cloud Scheduler job to test message publishing..."
gcloud scheduler jobs run publisher-job --location=$REGION
sleep 90
gcloud scheduler jobs run publisher-job --location=$REGION

echo "Generating a script to automate further commands..."
cat > automate_commands.sh <<EOF_END
#!/bin/bash
git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git
cd python-docs-samples/pubsub/streaming-analytics
pip install -U -r requirements.txt
python PubSubToGCS.py \
--project=$PROJECT_ID \
--region=$REGION \
--input_topic=projects/$PROJECT_ID/topics/$TOPIC_ID \
--output_path=gs://$BUCKET_NAME/samples/output \
--runner=DataflowRunner \
--window_size=2 \
--num_shards=2 \
--temp_location=gs://$BUCKET_NAME/temp
EOF_END

chmod +x automate_commands.sh

echo "Running the generated script inside a Docker container..."
docker run -it -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID -e BUCKET_NAME=$BUCKET_NAME -e PROJECT_ID=$PROJECT_ID -e REGION=$REGION -e TOPIC_ID=$TOPIC_ID -v $(pwd)/automate_commands.sh:/automate_commands.sh python:3.7 /bin/bash -c "/automate_commands.sh"

```
### Congratulations ðŸŽ‰ for completing the Lab !